{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Storing and Organizing Array Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### A little excursion on how to store, organize and handle large array data sets  ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### So far we have used:\n",
    "***NumPy*** files: binary arrays\n",
    "* [save binary: numpy.save()](https://numpy.org/doc/stable/reference/generated/numpy.save.html)\n",
    "* [save multiple arrays: numpy.savez()](https://numpy.org/doc/stable/reference/generated/numpy.savez.html)\n",
    "* [save multiple compressed: numpy.savez.compressed()](https://numpy.org/doc/stable/reference/generated/numpy.savez_compressed.html)\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "d1 = np.random.random(size = (1000,20))\n",
    "d2 = np.random.random(size = (1000,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to binary \n",
    "np.save(\"myArray.npy\", d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "d3=np.load(\"myArray.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check\n",
    "(d1==d3).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The HDF5 Data Container Format\n",
    "<img src=\"IMG/HDF_logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Hierarchical Data Format (HDF) is a set of file formats (HDF4, HDF5) designed to store and organize large amounts of data with APIs for many programming languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### HDF5 Structure\n",
    "<img src=\"IMG/hdf5-folder.png\" width=800>\n",
    "<font size=5>[Image Source: https://www.sphenisc.com/doku.php/software/development/hdf5-phdf5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### HDF5 Key Features:\n",
    "* POSIX-like syntax for internal data structures /path/to/resource\n",
    "    * folders\n",
    "    * meta data\n",
    "    * comments (even code)\n",
    "    * arrays \n",
    "* fast $n$-D data access \n",
    "* data compression\n",
    "* APIs for many programming languages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### In Python:\n",
    "* ***h5py***: http://docs.h5py.org/en/stable/index.html\n",
    "* ***HDF5 Docs:*** https://portal.hdfgroup.org/display/support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Creating a Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py #this is the HDF5 lib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#create some random data\n",
    "matrix1 = np.random.random(size = (1000,1000))\n",
    "matrix2 = np.random.random(size = (10000,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# write it to the same file - in two different arrays\n",
    "with h5py.File('hdf5_data.h5', 'w') as hdf: #note the write mode 'w'\n",
    "    hdf.create_dataset('dataset1', data=matrix1)\n",
    "    hdf.create_dataset('dataset2', data=matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of datasets in this file: \n",
      " ['dataset1', 'dataset2']\n",
      "Shape of dataset1: \n",
      " (10000, 100)\n"
     ]
    }
   ],
   "source": [
    "#opening, listing and reading files\n",
    "with h5py.File('hdf5_data.h5','r') as hdf:\n",
    "    ls = list(hdf.keys())\n",
    "    print('List of datasets in this file: \\n', ls)\n",
    "    data = hdf.get('dataset2') #here data is still some hdf5 object\n",
    "    dataset1 = np.array(data) #need to convert it into numpy\n",
    "    print('Shape of dataset1: \\n', dataset1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13969262, 0.40389118, 0.1609929 , ..., 0.74299666, 0.51093135,\n",
       "        0.76958518],\n",
       "       [0.72200094, 0.48671554, 0.55686445, ..., 0.76370932, 0.02393442,\n",
       "        0.53867917],\n",
       "       [0.09728551, 0.02873049, 0.50922071, ..., 0.86020885, 0.59273673,\n",
       "        0.02221163],\n",
       "       ...,\n",
       "       [0.95417126, 0.48365973, 0.96659977, ..., 0.60465433, 0.26512471,\n",
       "        0.06904724],\n",
       "       [0.70093947, 0.70191862, 0.5662833 , ..., 0.5934677 , 0.18419469,\n",
       "        0.62932529],\n",
       "       [0.6443913 , 0.2698956 , 0.59364799, ..., 0.17214277, 0.59086824,\n",
       "        0.90202334]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('hdf5_data.h5', 'r')\n",
    "ls = list(f.keys())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Array Slicing\n",
    "HDF5 support fancy array slicing - so we do not read all data just to get a slice: http://docs.h5py.org/en/latest/high/dataset.html#fancy-indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92149795, 0.46755   , 0.2979599 , ..., 0.46688399, 0.4718689 ,\n",
       "        0.27531739],\n",
       "       [0.9194666 , 0.98512064, 0.7199949 , ..., 0.43542322, 0.84954835,\n",
       "        0.53289261],\n",
       "       [0.68718177, 0.36023426, 0.56621732, ..., 0.00727894, 0.94489773,\n",
       "        0.55292583],\n",
       "       ...,\n",
       "       [0.4716062 , 0.24226123, 0.14986177, ..., 0.05926966, 0.21546288,\n",
       "        0.35239439],\n",
       "       [0.45492096, 0.07055182, 0.04093534, ..., 0.03254058, 0.78964478,\n",
       "        0.43074199],\n",
       "       [0.48345066, 0.42399808, 0.52992119, ..., 0.45597257, 0.81769769,\n",
       "        0.53001173]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = h5py.File('hdf5_data.h5', 'r')\n",
    "f['dataset1'][100:120,:] # this notation mostly follows numpy notation -> try different slices!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Creating Groups\n",
    "We can organize data in groups, just like in file systems where we have files (here datasets) in folders (here groups) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = np.random.random(size = (1000,1000))\n",
    "matrix2 = np.random.random(size = (1000,1000))\n",
    "matrix3 = np.random.random(size = (1000,1000))\n",
    "matrix4 = np.random.random(size = (1000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('hdf5_groups.h5', 'w') as hdf:\n",
    "    G1 = hdf.create_group('Group1')\n",
    "    G1.create_dataset('dataset1', data = matrix1)\n",
    "    G1.create_dataset('dataset4', data = matrix4)\n",
    " \n",
    "    G21 = hdf.create_group('Group2/SubGroup1')\n",
    "    G21.create_dataset('dataset3', data = matrix3)\n",
    "    \n",
    "    G22 = hdf.create_group('Group2/SubGroup2')\n",
    "    G22.create_dataset('dataset2', data = matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reading Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in the base directory: [('Group1', <HDF5 group \"/Group1\" (2 members)>), ('Group2', <HDF5 group \"/Group2\" (2 members)>)]\n",
      "Items in Group2: [('SubGroup1', <HDF5 group \"/Group2/SubGroup1\" (1 members)>), ('SubGroup2', <HDF5 group \"/Group2/SubGroup2\" (1 members)>)]\n",
      "Items in Group21: [('dataset3', <HDF5 dataset \"dataset3\": shape (1000, 1000), type \"<f8\">)]\n",
      "(1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('hdf5_groups.h5','r') as hdf:\n",
    "    base_items = list(hdf.items())\n",
    "    print('Items in the base directory:', base_items)\n",
    "    G2 = hdf.get('Group2')\n",
    "    G2_items = list(G2.items())\n",
    "    print('Items in Group2:', G2_items)\n",
    "    G21 = G2.get('/Group2/SubGroup1')\n",
    "    G21_items = list(G21.items())\n",
    "    print('Items in Group21:', G21_items)\n",
    "    dataset3 = np.array(G21.get('dataset3'))\n",
    "    print(dataset3.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is happening? Interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Compress Data\n",
    "HDF5 also support native data compression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = np.random.random(size = (1000,1000))\n",
    "matrix2 = np.random.random(size = (1000,1000))\n",
    "matrix3 = np.random.random(size = (1000,1000))\n",
    "matrix4 = np.random.random(size = (1000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('hdf5_groups_compressed.h5', 'w') as hdf:\n",
    "    G1 = hdf.create_group('Group1')\n",
    "    G1.create_dataset('dataset1', data = matrix1, compression=\"gzip\", compression_opts=9)\n",
    "    G1.create_dataset('dataset4', data = matrix4, compression=\"gzip\", compression_opts=9)\n",
    " \n",
    "    G21 = hdf.create_group('Group2/SubGroup1')\n",
    "    G21.create_dataset('dataset3', data = matrix3, compression=\"gzip\", compression_opts=9)\n",
    "    \n",
    "    G22 = hdf.create_group('Group2/SubGroup2')\n",
    "    G22.create_dataset('dataset2', data = matrix2, compression=\"gzip\", compression_opts=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attributes\n",
    "We can add meta information in form of attributes of files, groups and datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = np.random.random(size = (1000,1000))\n",
    "matrix2 = np.random.random(size = (10000,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the HDF5 file\n",
    "hdf = h5py.File('test.h5', 'w')\n",
    "\n",
    "# Create the datasets\n",
    "dataset1 = hdf.create_dataset('dataset1', data=matrix1)\n",
    "dataset2 = hdf.create_dataset('dataset2', data=matrix2)\n",
    "\n",
    "# Set attributes\n",
    "dataset1.attrs['CLASS'] = 'DATA MATRIX'\n",
    "dataset1.attrs['VERSION'] = '1.1'\n",
    "\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Read the HDF5 file\n",
    "hdf = h5py.File('test.h5', 'r')\n",
    "ls = list(hdf.keys())\n",
    "print('List of datasets in this file: \\n', ls)\n",
    "data = hdf.get('dataset1')\n",
    "dataset1 = np.array(data)\n",
    "print('Shape of dataset1: \\n', dataset1.shape)\n",
    "#read the attributes\n",
    "k = list(data.attrs.keys())\n",
    "v = list(data.attrs.values())\n",
    "print(k[0])\n",
    "print(v[0])\n",
    "print(data.attrs[k[0]])\n",
    "\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### more in the assignment..."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "livereveal": {
   "enable_chalkboard": true,
   "footer": "Janis Keuper - SS23",
   "header": "Data Engineering - Week 1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
